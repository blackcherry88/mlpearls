# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/100_langchain.ipynb.

# %% auto 0
__all__ = ['HfInference']

# %% ../../nbs/100_langchain.ipynb 4
from typing import Any, List, Dict, Mapping, Optional
from text_generation import InferenceAPIClient
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from pydantic import Extra, root_validator
from langchain.llms.utils import enforce_stop_tokens


class HfInference(LLM):
    
    client: Any
    model_name: str = ""
    
    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        values["client"] = InferenceAPIClient(values["model_name"])
        return values
        
    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "huggingface_inference_api"
        
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> str:
        text = self.client.generate(prompt).generated_text
        if stop is not None:
            # This is a bit hacky, but I can't figure out a better way to enforce
            # stop tokens when making calls to huggingface_hub.
            text = enforce_stop_tokens(text, stop)
        return text
        
        
        
