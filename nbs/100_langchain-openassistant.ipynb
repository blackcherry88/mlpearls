{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ca480",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp langchain.openassistant\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.langchain.huggingface import HfInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ffdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "model_name = \"OpenAssistant/oasst-sft-1-pythia-12b\"\n",
    "llm = HfInference(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937a14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input password: ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "serper_api_key = getpass('Input password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f98ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline, LLMMathChain\n",
    "from langchain.agents import  Tool, initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.utilities import GoogleSerperAPIWrapper, PythonREPL\n",
    "import gradio as gr\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "search = GoogleSerperAPIWrapper(serper_api_key=serper_api_key)\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events or the current state of the world\\nDO NOT USE THIS FOR SIMPLE QUESTIONS!\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about mathematically related topics\\nUSE ONLY FOR MATH RELATED QUESTIONS!\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Python REPL\",\n",
    "        func=python_repl.run,\n",
    "        description = \"A Python shell. Use this to execute python commands. \"\n",
    "        \"Input should be a valid python command. \"\n",
    "        \"If you want to see the result, you should print it out \"\n",
    "        \"with `print(...)`.\"\n",
    "    )\n",
    "]\n",
    "tool_names = [tool.name for tool in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"Consider that you are AI Assistant named AI, AI is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "You love to answer questions and you are very good at it.\n",
    "Assistant has access to the following tools:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"\"\"\n",
    "To use a tool, please use the following format:\n",
    "``\n",
    "Thought: Should I use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "``\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "``\n",
    "Thought: Should I use a tool? No\n",
    "AI: [your response here]\n",
    "``\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX = \"\"\"\n",
    "CHAT HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "Current time: {current_time}\n",
    "Knowledge date cutoff: 2021-09-01\n",
    "\n",
    "When answering a question, you MUST use the following language: {language}\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought: Should I use a tool?{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",# return_messages=True,\n",
    "                                  input_key=\"input\", \n",
    "                                  output_key='output', \n",
    "                                  ai_prefix='AI', \n",
    "                                  human_prefix='User')\n",
    "\n",
    "agent = initialize_agent(tools,\n",
    "                         llm,\n",
    "                         agent=\"conversational-react-description\",\n",
    "                         verbose=True,\n",
    "                         memory=memory,\n",
    "                         return_intermediate_steps=False,\n",
    "                         agent_kwargs={'input_variables': ['input', 'agent_scratchpad', 'chat_history', 'current_time', 'language'],\n",
    "                                       'prefix': PREFIX,\n",
    "                                       'format_instructions': INSTRUCTIONS,\n",
    "                                       'suffix': SUFFIX})\n",
    "agent.agent.llm_chain.verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    depth = 0\n",
    "    with gr.Blocks() as app:\n",
    "        chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "        state = gr.State([])\n",
    "\n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "\n",
    "        def user(user_message, history):\n",
    "            if history is None:\n",
    "                history = \"\"\n",
    "            return \"\", history + [[user_message, None]]\n",
    "\n",
    "        def bot(history):\n",
    "            nonlocal depth\n",
    "            depth += 1\n",
    "            prompt = history[-1][0]\n",
    "            res = agent({\"input\":prompt,\n",
    "                         \"current_time\":datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                         \"language\": \"English\"})\n",
    "\n",
    "            response = res['output']\n",
    "            history[-1][1] = response\n",
    "\n",
    "            # free up some memory if we have too many messages\n",
    "            if depth > 8 or len(memory.buffer) > 512:\n",
    "                memory.chat_memory.messages.pop(0)\n",
    "            return history\n",
    "\n",
    "        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "                bot, chatbot, chatbot)\n",
    "\n",
    "    app.queue()\n",
    "    app.launch(share=False, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a2776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mConsider that you are AI Assistant named AI, AI is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "You love to answer questions and you are very good at it.\n",
      "Assistant has access to the following tools:\n",
      "\n",
      "> Search: useful for when you need to answer questions about current events or the current state of the world\n",
      "DO NOT USE THIS FOR SIMPLE QUESTIONS!\n",
      "> Calculator: useful for when you need to answer questions about mathematically related topics\n",
      "USE ONLY FOR MATH RELATED QUESTIONS!\n",
      "> Python REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the result, you should print it out with `print(...)`.\n",
      "\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "``\n",
      "Thought: Should I use a tool? Yes\n",
      "Action: the action to take, should be one of [Search, Calculator, Python REPL]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "``\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "``\n",
      "Thought: Should I use a tool? No\n",
      "AI: [your response here]\n",
      "``\n",
      "\n",
      "\n",
      "\n",
      "CHAT HISTORY:\n",
      "\n",
      "\n",
      "Current time: 2023-05-07 15:46:09\n",
      "Knowledge date cutoff: 2021-09-01\n",
      "\n",
      "When answering a question, you MUST use the following language: English\n",
      "Begin!\n",
      "Question: What is 2+3?\n",
      "Thought: Should I use a tool?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\routes.py\", line 412, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\blocks.py\", line 1299, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\blocks.py\", line 1021, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Temp\\ipykernel_10680\\783289879.py\", line 19, in bot\n",
      "    res = agent({\"input\":prompt,\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\agents\\agent.py\", line 905, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\agents\\agent.py\", line 749, in _take_next_step\n",
      "    raise e\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\agents\\agent.py\", line 742, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\agents\\agent.py\", line 426, in plan\n",
      "    return self.output_parser.parse(full_output)\n",
      "  File \"C:\\local\\Anaconda3\\envs\\py310\\lib\\site-packages\\langchain\\agents\\conversational\\output_parser.py\", line 23, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
      "langchain.schema.OutputParserException: Could not parse LLM output: ` Yes\n",
      "Action: Calculator`\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e003d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
