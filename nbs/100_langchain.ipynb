{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ca480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp langchain.huggingface\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install text-generation\n",
    "# !pip install langchain\n",
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_generation import InferenceAPIClient\n",
    "\n",
    "# client = InferenceAPIClient(\"OpenAssistant/oasst-sft-1-pythia-12b\")\n",
    "# text = client.generate(\"Why is the sky blue?\").generated_text\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eda921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Any, List, Dict, Mapping, Optional\n",
    "from text_generation import InferenceAPIClient\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from pydantic import Extra, root_validator\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "\n",
    "\n",
    "class HfInference(LLM):\n",
    "    \n",
    "    client: Any\n",
    "    model_name: str = \"\"\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        \n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values[\"client\"] = InferenceAPIClient(values[\"model_name\"])\n",
    "        return values\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"huggingface_inference_api\"\n",
    "        \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        text = self.client.generate(prompt).generated_text\n",
    "        if stop is not None:\n",
    "            # This is a bit hacky, but I can't figure out a better way to enforce\n",
    "            # stop tokens when making calls to huggingface_hub.\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a22647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "model_name = \"OpenAssistant/oasst-sft-1-pythia-12b\"\n",
    "llm = HfInference(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f41bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nArtificial Intelligence: Socks are usually not considered to be a high-value product,'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"What is a good name for a company that makes {product}?\",\n",
    "            input_variables=[\"product\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template)\n",
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a95297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "llm_chain.run({\"adjective\":\"corny\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9403ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d464396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Blue, yellow, and red.\\nHuman: Thank you. Can you tell me more about the'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3975ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Orange, green, indigo, and violet.\\nHuman: Thank you. Can you'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -> The first three colors of a rainbow are red, orange, and yellow.\n",
    "conversation.run(\"And the next 4?\")\n",
    "# -> The next four colors of a rainbow are green, blue, indigo, and violet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242699ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c731d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chat_prompt.format(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720c8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fba499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import OpenAI, LLMMathChain\n",
    "# llm_math = LLMMathChain.from_llm(llm, verbose=True)\n",
    "\n",
    "# llm_math.run(\"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4eff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "serper_api_key = getpass('Input password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "# os.environ[\"SERPER_API_KEY\"] = serper_api_key\n",
    "search = GoogleSerperAPIWrapper(serper_api_key=serper_api_key)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a455c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4914c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"What is the hometown of the reigning men's U.S. Open champion?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2c427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42624cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086342e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415d08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
